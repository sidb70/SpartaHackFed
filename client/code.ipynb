{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/sidb/.kaggle/kaggle.json'\n",
      "Downloading loan-defaulter.zip to /Users/sidb/Development/SpartaHack9/SpartaHackFed/client\n",
      "100%|███████████████████████████████████████▊| 112M/112M [00:06<00:00, 18.9MB/s]\n",
      "100%|████████████████████████████████████████| 112M/112M [00:06<00:00, 18.1MB/s]\n",
      "Archive:  loan-defaulter.zip\n",
      "  inflating: application_data.csv    \n",
      "  inflating: columns_description.csv  \n",
      "  inflating: previous_application.csv  \n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle unzip\n",
    "!kaggle datasets download gauravduttakiit/loan-defaulter/\n",
    "!unzip loan-defaulter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uzair\\AppData\\Local\\Temp\\ipykernel_20048\\113819065.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "application_data = pd.read_csv('application_data.csv')\n",
    "previous_application = pd.read_csv('previous_application.csv')\n",
    "columns_description = pd.read_csv(r'columns_description.csv',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf = pd.merge(application_data,previous_application,on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uzair\\AppData\\Local\\Temp\\ipykernel_4628\\2925456544.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "import pandas as pd\n",
    "application_data = pd.read_csv('data/application_data.csv')\n",
    "previous_application = pd.read_csv('data/previous_application.csv')\n",
    "# columns_description = pd.read_csv(r'columns_description.csv',skiprows=1)\n",
    "\n",
    "mergeddf =  pd.merge(application_data,previous_application,on='SK_ID_CURR')\n",
    "data_A = mergeddf.sample(10000)\n",
    "data_A.to_csv('data/loan_data_A.csv', index=False)\n",
    "\n",
    "data_B = mergeddf.sample(10000)\n",
    "data_B.to_csv('data/loan_data_B.csv', index=False)\n",
    "\n",
    "data_C = mergeddf.sample(10000)\n",
    "data_C.to_csv('data/loan_data_C.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf_sample = mergeddf.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows\n",
    "mergeddf_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with more than 50% missing values\n",
    "mergeddf_sample = mergeddf_sample.dropna(thresh=0.5*len(mergeddf_sample), axis=1)\n",
    "\n",
    "# convert categorical columns to numerical\n",
    "mergeddf_sample = pd.get_dummies(mergeddf_sample)\n",
    "\n",
    "# convert all columns to float\n",
    "mergeddf_sample = mergeddf_sample.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# get features and labels. drop target column\n",
    "X = mergeddf_sample.drop(['TARGET'],axis=1)\n",
    "X = num_pipeline.fit_transform(X)\n",
    "mergeddf_sample_num = num_pipeline.fit_transform(mergeddf_sample)\n",
    "\n",
    "y = mergeddf_sample['TARGET']\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.731346070766449\n",
      "Epoch 1: train loss: 0.7027109265327454\n",
      "Epoch 2: train loss: 0.6751503944396973\n",
      "Epoch 3: train loss: 0.6484552621841431\n",
      "Epoch 4: train loss: 0.6224964261054993\n",
      "Epoch 5: train loss: 0.5971705317497253\n",
      "Epoch 6: train loss: 0.5723193883895874\n",
      "Epoch 7: train loss: 0.5478552579879761\n",
      "Epoch 8: train loss: 0.5237233638763428\n",
      "Epoch 9: train loss: 0.49992331862449646\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(X_train.shape[1], 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# convert data to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.squeeze(torch.from_numpy(y_test.to_numpy()).float())\n",
    "\n",
    "# train model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor).squeeze()\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1817,   10],\n",
       "       [ 165,    8]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f'Accuracy score: {accuracy_score(y_test,y_pred)}')\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store model weights with joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
