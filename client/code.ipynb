{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Using cached kaggle-1.6.3-py3-none-any.whl\n",
      "Collecting unzip\n",
      "  Downloading unzip-1.0.0.tar.gz (704 bytes)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (4.66.1)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Obtaining dependency information for python-slugify from https://files.pythonhosted.org/packages/09/49/e05adaaa2d8604b7cfbce81af14c7a48c67d70a6e06cb47473c9673267db/python_slugify-8.0.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading python_slugify-8.0.2-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.1.0)\n",
      "Requirement already satisfied: bleach in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/78.2 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/78.2 kB ? eta -:--:--\n",
      "     ----------------------------- -------- 61.4/78.2 kB 825.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 78.2/78.2 kB 870.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kaggle) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\uzair\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Downloading python_slugify-8.0.2-py2.py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: unzip\n",
      "  Building wheel for unzip (pyproject.toml): started\n",
      "  Building wheel for unzip (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for unzip: filename=unzip-1.0.0-py3-none-any.whl size=1287 sha256=9852cfd44e80726d151fd2cc9bfda2899387ed1d0c060b4fddbb80151ec9a1be\n",
      "  Stored in directory: c:\\users\\uzair\\appdata\\local\\pip\\cache\\wheels\\fb\\5b\\81\\0f3e1e533b52883f88ab978178c15627a4fce4c13f74911dce\n",
      "Successfully built unzip\n",
      "Installing collected packages: unzip, text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.6.3 python-slugify-8.0.2 text-unidecode-1.3 unzip-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading loan-defaulter.zip to c:\\Users\\uzair\\Documents\\repos\\spartahack\\notebook\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/112M [00:00<?, ?B/s]\n",
      "  1%|          | 1.00M/112M [00:00<00:12, 9.45MB/s]\n",
      "  3%|▎         | 3.00M/112M [00:00<00:08, 14.1MB/s]\n",
      "  4%|▍         | 5.00M/112M [00:00<00:07, 14.2MB/s]\n",
      "  6%|▌         | 7.00M/112M [00:00<00:09, 12.2MB/s]\n",
      "  8%|▊         | 9.00M/112M [00:00<00:09, 11.9MB/s]\n",
      " 10%|▉         | 11.0M/112M [00:00<00:09, 11.4MB/s]\n",
      " 12%|█▏        | 13.0M/112M [00:01<00:09, 11.4MB/s]\n",
      " 13%|█▎        | 15.0M/112M [00:01<00:09, 10.9MB/s]\n",
      " 15%|█▌        | 17.0M/112M [00:01<00:08, 12.4MB/s]\n",
      " 17%|█▋        | 19.0M/112M [00:01<00:07, 13.5MB/s]\n",
      " 19%|█▊        | 21.0M/112M [00:01<00:07, 13.0MB/s]\n",
      " 20%|██        | 23.0M/112M [00:01<00:06, 14.2MB/s]\n",
      " 22%|██▏       | 25.0M/112M [00:02<00:05, 15.6MB/s]\n",
      " 24%|██▍       | 27.0M/112M [00:02<00:05, 16.1MB/s]\n",
      " 26%|██▌       | 29.0M/112M [00:02<00:05, 15.7MB/s]\n",
      " 28%|██▊       | 31.0M/112M [00:02<00:05, 16.2MB/s]\n",
      " 29%|██▉       | 33.0M/112M [00:02<00:05, 15.9MB/s]\n",
      " 31%|███       | 35.0M/112M [00:02<00:05, 15.3MB/s]\n",
      " 33%|███▎      | 37.0M/112M [00:02<00:05, 15.6MB/s]\n",
      " 35%|███▍      | 39.0M/112M [00:02<00:04, 16.1MB/s]\n",
      " 36%|███▋      | 41.0M/112M [00:03<00:04, 15.4MB/s]\n",
      " 38%|███▊      | 43.0M/112M [00:03<00:04, 14.8MB/s]\n",
      " 40%|████      | 45.0M/112M [00:03<00:04, 15.3MB/s]\n",
      " 42%|████▏     | 47.0M/112M [00:03<00:04, 16.3MB/s]\n",
      " 44%|████▎     | 49.0M/112M [00:03<00:04, 15.8MB/s]\n",
      " 45%|████▌     | 51.0M/112M [00:03<00:04, 15.7MB/s]\n",
      " 47%|████▋     | 53.0M/112M [00:03<00:03, 15.7MB/s]\n",
      " 49%|████▉     | 55.0M/112M [00:03<00:03, 16.8MB/s]\n",
      " 52%|█████▏    | 58.0M/112M [00:04<00:03, 16.9MB/s]\n",
      " 54%|█████▍    | 61.0M/112M [00:04<00:02, 19.6MB/s]\n",
      " 56%|█████▌    | 63.0M/112M [00:04<00:02, 18.4MB/s]\n",
      " 59%|█████▊    | 66.0M/112M [00:04<00:02, 19.7MB/s]\n",
      " 61%|██████    | 68.0M/112M [00:04<00:02, 17.5MB/s]\n",
      " 62%|██████▏   | 70.0M/112M [00:04<00:02, 16.7MB/s]\n",
      " 64%|██████▍   | 72.0M/112M [00:04<00:02, 16.3MB/s]\n",
      " 66%|██████▌   | 74.0M/112M [00:05<00:03, 10.5MB/s]\n",
      " 68%|██████▊   | 76.0M/112M [00:05<00:03, 11.0MB/s]\n",
      " 69%|██████▉   | 78.0M/112M [00:05<00:02, 12.3MB/s]\n",
      " 71%|███████   | 80.0M/112M [00:05<00:02, 13.7MB/s]\n",
      " 74%|███████▍  | 83.0M/112M [00:05<00:01, 15.6MB/s]\n",
      " 76%|███████▌  | 85.0M/112M [00:06<00:01, 16.2MB/s]\n",
      " 77%|███████▋  | 87.0M/112M [00:06<00:01, 16.8MB/s]\n",
      " 79%|███████▉  | 89.0M/112M [00:06<00:01, 16.1MB/s]\n",
      " 81%|████████  | 91.0M/112M [00:06<00:01, 15.8MB/s]\n",
      " 83%|████████▎ | 93.0M/112M [00:06<00:01, 15.8MB/s]\n",
      " 85%|████████▍ | 95.0M/112M [00:06<00:01, 15.8MB/s]\n",
      " 86%|████████▋ | 97.0M/112M [00:06<00:00, 16.5MB/s]\n",
      " 88%|████████▊ | 99.0M/112M [00:06<00:00, 16.2MB/s]\n",
      " 90%|████████▉ | 101M/112M [00:07<00:00, 14.4MB/s] \n",
      " 92%|█████████▏| 103M/112M [00:07<00:00, 13.6MB/s]\n",
      " 94%|█████████▍| 106M/112M [00:07<00:00, 15.6MB/s]\n",
      " 96%|█████████▌| 108M/112M [00:07<00:00, 16.2MB/s]\n",
      " 98%|█████████▊| 110M/112M [00:07<00:00, 16.6MB/s]\n",
      "100%|█████████▉| 112M/112M [00:07<00:00, 16.7MB/s]\n",
      "100%|██████████| 112M/112M [00:07<00:00, 15.0MB/s]\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle unzip\n",
    "!kaggle datasets download gauravduttakiit/loan-defaulter/\n",
    "!unzip loan-defaulter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uzair\\AppData\\Local\\Temp\\ipykernel_20048\\113819065.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "application_data = pd.read_csv('application_data.csv')\n",
    "previous_application = pd.read_csv('previous_application.csv')\n",
    "columns_description = pd.read_csv(r'columns_description.csv',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf = pd.merge(application_data,previous_application,on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeddf_sample = mergeddf.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows\n",
    "mergeddf_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with more than 50% missing values\n",
    "mergeddf_sample = mergeddf_sample.dropna(thresh=0.5*len(mergeddf_sample), axis=1)\n",
    "\n",
    "# convert categorical columns to numerical\n",
    "mergeddf_sample = pd.get_dummies(mergeddf_sample)\n",
    "\n",
    "# convert all columns to float\n",
    "mergeddf_sample = mergeddf_sample.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# get features and labels. drop target column\n",
    "X = mergeddf_sample.drop(['TARGET'],axis=1)\n",
    "X = num_pipeline.fit_transform(X)\n",
    "mergeddf_sample_num = num_pipeline.fit_transform(mergeddf_sample)\n",
    "\n",
    "y = mergeddf_sample['TARGET']\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.731346070766449\n",
      "Epoch 1: train loss: 0.7027109265327454\n",
      "Epoch 2: train loss: 0.6751503944396973\n",
      "Epoch 3: train loss: 0.6484552621841431\n",
      "Epoch 4: train loss: 0.6224964261054993\n",
      "Epoch 5: train loss: 0.5971705317497253\n",
      "Epoch 6: train loss: 0.5723193883895874\n",
      "Epoch 7: train loss: 0.5478552579879761\n",
      "Epoch 8: train loss: 0.5237233638763428\n",
      "Epoch 9: train loss: 0.49992331862449646\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(X_train.shape[1], 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# convert data to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.squeeze(torch.from_numpy(y_test.to_numpy()).float())\n",
    "\n",
    "# train model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train_tensor).squeeze()\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1817,   10],\n",
       "       [ 165,    8]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f'Accuracy score: {accuracy_score(y_test,y_pred)}')\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store model weights with joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
